{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training Networks: Backpropagation\n",
    "\n",
    "### Recall Stochastic Gradient Descent\n",
    "\n",
    "---\n",
    "> __SGD and Friends__\n",
    "\n",
    "> * Define error in terms of some params: ${Err = f(p_1, p_2, p_3, ...)}$\n",
    "\n",
    "> * Look at error gradient, ${\\nabla Err}$ with components ${{\\partial Err \\over \\partial p_1},{\\partial Err \\over \\partial p_2}, ...}$\n",
    "\n",
    "> * Update params per learning rate (${\\eta}$)\n",
    "\n",
    "> * Repeat...\n",
    "\n",
    "---\n",
    "\n",
    "With a 1-layer network, we could use SGD or a related algorithm to derive the weights, since the error depends directly on those weights.\n",
    "\n",
    "With a deeper network, we have a couple of challenges:\n",
    "\n",
    "* The error is computed from the final layer, so the gradient of the error doesn't tell us immediately about problems in other-layer weights\n",
    "* There are -- even in our 2-layer diamonds model -- thousands of weights. Each of those weights may need to move a little at a time, and we have to watch out for underflow or undersignificance situations.\n",
    "\n",
    "__In a deep network, the nth layer errors are \"caused\" by errors in the (n-1)th layer and are detected in the errors in the (n+1)th layer__\n",
    "\n",
    "### The insight is to iteratively calculate errors, one layer at a time, starting at the output. This is called Backpropagation. It is neither magical nor surprising. The challenge is just doing it fast.\n",
    "\n",
    "<img src=\"http://i.imgur.com/bjlYwjM.jpg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Since we are differentiating a composition of functions,<br>this is just the \"Chain Rule\" of high school calculus\n",
    "\n",
    "## Don't be waylaid by backpropagation!\n",
    "\n",
    "There's *less* here than meets the eye. I've included some links to more detailed posts below, so that you can help solidify your intuition by making the calculations concrete. However, don't let any individual article or explanation distract you from the very simple concept.\n",
    "\n",
    "Many presentations about backprop are confused or cluttered by one or more of the following problems -- don't let them catch you off guard :)\n",
    "\n",
    "* Playing fast/loose with vector calculus and matrix notation (don't get caught up in the notation, it's not the most important thing)\n",
    "* Assuming a specific neuron activation function, then using concrete formulas based on differentiating that particular function (don't worry about the specific function -- backprop needs to work in general, and inserting a specific derivative confuses things)\n",
    "* Mixing implementation techniques (how to calculate these derivatives conveniently or quickly, which is another issue)\n",
    "\n",
    "#### With the warnings out of the way, here are some resources on backprop:\n",
    "* https://sebastianraschka.com/faq/docs/visual-backpropagation.html (Overview)\n",
    "* http://neuralnetworksanddeeplearning.com/chap2.html (Detail)\n",
    "* https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ (Concrete example with numbers)\n",
    "* http://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/ (Coding it)\n",
    "\n",
    "*You will likely never need to code backprop, let alone implement an optimized general version. Do it for fun if you like, but don't let it keep you out of the game.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## If this Works Now, Why Didn't it Work 20 Years Ago?\n",
    "\n",
    "After all, backprop is decades old... at least to the 1970s, and Geoff Hinton used it successfully in the 1980s.\n",
    "\n",
    "The general challenges were \n",
    "\n",
    "* Diffusion of information through the sheer quantity of parameters\n",
    "* Butterfly effect of small fluctuations through the system\n",
    "* Flat/vanishing/unstable gradients\n",
    "* Saturating units\n",
    "\n",
    "The improvements are largely not changes to theory but a ton of incremental, practical fixes starting with lots of horsepower and lots of data.\n",
    "\n",
    "Beyond those obvious pieces, we discuss major learnings next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How TensorFlow et al. Make this Work Well\n",
    "\n",
    "There are a few approaches to differentiation\n",
    "\n",
    "* Symbolic differentiation -- machine version of what we learned in high school\n",
    "* Numeric differentiation -- calculate approximate slopes\n",
    "* __*Autodifferentiation*__ -- use the chain rule to build derivatives as we build our output function\n",
    "\n",
    "Consider the calculation $${\\begin{aligned}z&=f(x_{1},x_{2})\\\\&=x_{1}x_{2}+\\sin x_{1}\\\\&=w_{1}w_{2}+\\sin w_{1}\\\\&=w_{3}+w_{4}\\\\&=w_{5}\\end{aligned}}$$\n",
    "\n",
    "We can track the derivatives as we combine and compose functions:\n",
    "\n",
    "<img src=\"images/autodiff.svg\">\n",
    "\n",
    "One additional trick is that we can do this forward or backward and it requires less computation to do this backward, also called \"reverse autodifferentiation\":\n",
    "\n",
    "<img src=\"images/reverse.svg\">\n",
    "\n",
    "(Why is there less computation in reverse? This is not generally true, but for our machine learning models where we have many parameters in, and few or just one parameter out, then this case holds; if we were computing from ${ \\Bbb R^n \\to \\Bbb R^m }$ with ${ m \\gg n}$ then we would want forward-mode autodiff.)\n",
    "\n",
    "#### In TensorFlow, this code has been moved to C++ and you can find it here:\n",
    "\n",
    "* https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients\n",
    "* https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/framework\n",
    "\n",
    "You'll probably never need this detail unless you are adding a new custom Op to TF (in which case you'll probably want to add gradient support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = tf.placeholder(\"float\")\n",
    "y = tf.placeholder(\"float\")\n",
    "\n",
    "m = tf.Variable([1.0], name=\"m-slope-coefficient\")\n",
    "b = tf.Variable([1.0], name=\"b-intercept\")\n",
    "\n",
    "y_model = tf.multiply(x, m) + b\n",
    "\n",
    "error = tf.square(y - y_model)\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_op = opt.minimize(error)\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    for i in range(10):\n",
    "        x_value = np.random.rand()\n",
    "        y_value = x_value * 2 + 6\n",
    "        session.run(train_op, feed_dict={x: x_value, y: y_value})\n",
    "        \n",
    "        grads_and_vars = opt.compute_gradients(error, [m, b])\n",
    "        for v in grads_and_vars:\n",
    "            print (v[0].eval(feed_dict={x: x_value, y: y_value}), v[1].name)\n",
    "        print(\"------\")\n",
    "\n",
    "    out = session.run([m, b])\n",
    "    print(out)\n",
    "    print(\"Model: {r:.3f}x + {s:.3f}\".format(r=out[0][0], s=out[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "> __ASIDE__ The current resurgence of neural networks and deep learning started in the early 2000s, when Geoffrey Hinton demonstrated an alternative approach to training multiple layers in a deep neural network.\n",
    "\n",
    "> Geoff Hinton used __Restricted Boltzmann Machines__ to pre-train weights one layer at a time. The RBM would learn weights to produce distribution for the n+1 layer at minimum cost relative to a given a distribution in the n layer. Hinton used a procedure called contrastive divergence.\n",
    "\n",
    "> By doing this one layer at a time, reasonable weights could be derived for a network as a whole or to make backpropagation into a tractable fine-tuning step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training Practicalities (Part I)\n",
    "\n",
    "## High-capacity topologies (deep vs. wide)\n",
    "* Goal is to make training time shorter/tractable\n",
    "\n",
    "In the terminal, start `training-error-plot.py` \n",
    "\n",
    "This script is similar to the last one, but has some code on the end to plot the error when we're finished.\n",
    "\n",
    "---\n",
    "\n",
    "## Suitable Activation Functions\n",
    "\n",
    "---\n",
    "\n",
    "## Increasing Data Set Size\n",
    "### \"Unreasonable Effectiveness of Data\"\n",
    "- Peter Norvig \n",
    "(https://www.youtube.com/watch?v=yvDCzhbjYWs)\n",
    "\n",
    "* More data\n",
    "* Slightly different data\n",
    "    * Can we increase our data set in a way that parallels the sort of datasets human learn to work with?\n",
    "    * Images: Translate, Rotate, Skew, Stretch, Blur...\n",
    "    * Sound: Faster, slower, pitch change...\n",
    "    * Self-Driving Cars and Grand Theft Auto\n",
    "    * Noise...\n",
    "\n",
    "---\n",
    "\n",
    "## Proper Weight Initialization\n",
    "\n",
    "Lab: in the terminal, change the initialization of weights in your network. Start with 'zero'.\n",
    "\n",
    "While it's running, take a look at the options in Keras: https://keras.io/initializations/\n",
    "\n",
    "* Why does the weight initialization matter?\n",
    "* Where do we start in our activations? Where do we need to \"move to\"?\n",
    "* What happens to the magnitude of the gradient as we backpropagate?\n",
    "\n",
    "How is this connected to the number of weights (and neurons)?\n",
    "\n",
    "*By now you should have some empirical observations from the 'zero' initialization. Try it again with 'uniform'.*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Hardware Support\n",
    "\n",
    "E.g., fast, low-precision GPU math https://www.theregister.co.uk/2016/09/13/nvidia_p4_p40_gpu_ai/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Overfitting and Regularizing\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "---\n",
    "\n",
    "### Weight Decay / L2 \n",
    "\n",
    "Lagrangian formulation\n",
    "\n",
    "---\n",
    "\n",
    "### L1 / Lasso\n",
    "\n",
    "Does this exacerbate challenges with backprop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Dropout and DropConnect\n",
    "\n",
    "Remove connections of neurons -- results in \"less specialization\" of neurons.\n",
    "\n",
    "We can add this in Keras by adding a `Dropout` object with a fraction of the units to drop.\n",
    "\n",
    "Add a 50% dropout before the last hidden layer.\n",
    "\n",
    "There are a variety of interpretations of Dropout/DropConnect, including the idea that it forces a bunch of ensembles, as well as that in the large scale, it just changes the weights. For more detail, see http://www.deeplearningbook.org/contents/regularization.html\n",
    "\n",
    "In your lab experiment, if you look at the resulting error as well as the error plot, you should see that dropout\n",
    "* made things worse\n",
    "* made the training less smooth\n",
    "\n",
    "<img src=\"images/dropout.png\" width=500>\n",
    "\n",
    "Why? When might this help instead of hinder?\n",
    "\n",
    "(We'll use this again in another module soon ... so if you're not convinced yet, you'll get to try it)\n",
    "\n",
    "---\n",
    "\n",
    "## Batch normalization\n",
    "\n",
    "Address skew across training data batches, which gets amplified through deep networks.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Adversarial Training\n",
    "\n",
    "Dataset augmentation with intentionally problematic data samples.\n",
    "\n",
    "This may help, but it turns out not to solve certain key robustness and security concerns -- we'll revisit those apsects later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
