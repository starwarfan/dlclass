{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Artificial Neural Network - Perceptron\n",
    "\n",
    "The field of artificial neural networks started out with an electromechanical binary unit called a perceptron.\n",
    "\n",
    "The perceptron took a weighted set of input signals and chose an ouput state (on/off or high/low) based on a threshold.\n",
    "\n",
    "<img src=\"http://i.imgur.com/c4pBaaU.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If the output isn't right, we can adjust the weights, threshold, or bias ($x_0$ above)\n",
    "\n",
    "The model was inspired by discoveries about the neurons of animals, so hopes were quite high that it could lead to a sophisticated machine. This model can be extended by adding multiple neurons in parallel. And we can use linear output instead of a threshold if we like for the output.\n",
    "\n",
    "If we were to do so, the output would look like ${x \\cdot w} + w_0$ (this is where the vector multiplication and, eventually, matrix multiplication, comes in)\n",
    "\n",
    "When we look at the math this way, we see that despite this being an interesting model, it's really just a fancy linear calculation.\n",
    "\n",
    "If we compose these, we'll still get a linear model. And, in fact, the proof that this model -- being linear -- could not solve any problems whose solution was nonlinear ... led to the first of several \"AI / neural net winters\" when the excitement was quickly replaced by disappointment, and most research was abandoned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Linear Perceptron\n",
    "\n",
    "We'll get to the non-linear part, but the linear perceptron model is a great way to warm up and bridge the gap from traditional linear regression to the neural-net flavor.\n",
    "\n",
    "Let's look at a problem -- the diamonds dataset from R -- and analyze it using two traditional methods in Scikit-Learn, and then we'll start attacking it with neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "input_file = \"data/diamonds.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file, header = 0)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df = pd.get_dummies(df, prefix=['cut_', 'color_', 'clarity_'])\n",
    "\n",
    "y = df.iloc[:,3:4].as_matrix().flatten()\n",
    "y.flatten()\n",
    "\n",
    "X = df.drop(df.columns[3], axis=1).as_matrix()\n",
    "np.shape(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "dt = DecisionTreeRegressor(random_state=0, max_depth=10)\n",
    "model = dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE %f\" % np.sqrt(mean_squared_error(y_test, y_pred)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lr = linear_model.LinearRegression()\n",
    "linear_model = lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linear_model.predict(X_test)\n",
    "print(\"RMSE %f\" % np.sqrt(mean_squared_error(y_test, y_pred)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have a baseline, let's build a neural network -- linear at first -- and go further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network with Keras\n",
    "\n",
    "### Keras is a High-Level Library for Neural Networks and Deep Learning\n",
    "\n",
    "#### \"*Being able to go from idea to result with the least possible delay is key to doing good research.*\"\n",
    "Maintained by Francois Chollet at Google, it provides\n",
    "\n",
    "* High level APIs\n",
    "* Pluggable backends for Theano and TensorFlow\n",
    "* CPU/GPU support\n",
    "* The now-officially-endorsed high-level wrapper for TensorFlow\n",
    "* Model persistence and other niceties\n",
    "* JavaScript version (!)\n",
    "* Interop with further frameworks, like DeepLearning4J\n",
    "\n",
    "Well, with all this, why would you ever *not* use Keras? If you're implementing something new and low level you probably need to add it down in the TensorFlow layer.\n",
    "\n",
    "Another way to look at it:\n",
    "\n",
    "TensorFlow Ops -> TensorFlow Procedures -> Keras \n",
    "\n",
    "is a little like\n",
    "\n",
    "Assembly Code -> C -> Python \n",
    "\n",
    "The metaphor here fails because Python has its own VM and so runs code quite differently from C, whereas Keras is a fairly thin wrapper over its backends.\n",
    "\n",
    "### We'll build a \"Dense Feed-Forward Shallow\" Network:\n",
    "(the number of units in the following diagram does not exactly match ours)\n",
    "<img src=\"http://i.imgur.com/LqyPRBd.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=26, kernel_initializer='normal', activation='linear'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=200)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print\n",
    "print(\"root %s: %f\" % (model.metrics_names[1], np.sqrt(scores[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Ouch, not so great!\n",
    "\n",
    "Well, the neural network model is a bit of a different approach.\n",
    "\n",
    "Let's do three things. \n",
    "\n",
    "First, __what is an epoch? what is a batch?__\n",
    "\n",
    "Second, let's look at the error ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Third, let's go outside of Jupyter so that we can run a long-running training, and not lock up our browser.\n",
    "\n",
    "Open a terminal in the courseware folder...\n",
    "* Make your deep learning Python environment active with `source activate dl`\n",
    "* `cd` into the scripts folder\n",
    "* and run `python keras-diamonds.py`\n",
    "\n",
    "This will take about 3 minutes to converge to the same performance we got more or less instantly with our sklearn linear regression :)\n",
    "\n",
    "Once it's started, let's look at the source code and talk about that.\n",
    "\n",
    "---\n",
    "\n",
    "> __ASIDE: How exactly is this training working?__ Don't worry, we're going to come back to this in more detail in a little while!\n",
    "\n",
    "---\n",
    "Let's also make the connection from Keras down to Tensorflow.\n",
    "\n",
    "We used a Keras class called Dense, which represents a \"fully-connected\" layer of -- in this case -- linear perceptrons. Let's look at the source code to that, just to see that there's no mystery.\n",
    "\n",
    "`https://github.com/fchollet/keras/blob/master/keras/layers/core.py`\n",
    "\n",
    "It calls down to the \"back end\" by calling `output = K.dot(x, self.W)`\n",
    "\n",
    "`K` represents the pluggable backend wrapper. You can trace K.dot on Tensorflow by looking at\n",
    "\n",
    "`https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py`\n",
    "\n",
    "Look for `def dot(x, y):` and look right toward the end of the method. The math is done by calling `tf.matmul(x, y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ok so we've come up with a very slow way to perform a linear regression. \n",
    "\n",
    "### *Welcome to Neural Networks in the 1960s!*\n",
    "\n",
    "---\n",
    "\n",
    "### Watch closely now because this is where the magic happens...\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/Hw5LkPYy9yfVS/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Non-Linearity + Perceptron = Universal Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Where does the non-linearity fit in?\n",
    "\n",
    "* We start with the inputs to a perceptron -- these could be from source data, for example.\n",
    "* We multiply each input by its respective weight, which gets us the ${x \\cdot w}$\n",
    "* Then add the \"bias\" -- basically an extra learnable parameter, to get ${x \\cdot w} + b$\n",
    "    * This value (so far) is sometimes called the \"pre-activation\"\n",
    "* Now, apply a non-linear \"activation function\" to this value, such as the logistic sigmoid, ${1 \\over {1 + e^{-x} } }$\n",
    "* This is often written as ${\\sigma(x)}$ where x is the pre-activation\n",
    "\n",
    "### Now the network can \"learn\" non-linear functions\n",
    "\n",
    "To gain some intuition, consider that where the sigmoid is close to 1, we can think of that neuron as being \"on\" or activated, giving a specific output. When close to zero, it is \"off.\" \n",
    "\n",
    "So each neuron is a bit like a switch. If we have enough of them, we can theoretically express arbitrarily many different signals. \n",
    "\n",
    "In some ways this is like the original artificial neuron, with the thresholding output -- the main difference is that the sigmoid gives us a smooth (arbitrarily differentiable) output that we can optimize over using gradient descent to learn the weights. \n",
    "\n",
    "### Where does the signal \"go\" from these neurons?\n",
    "\n",
    "Assume that we want to get a classfication output from these activations. If we have lots of neurons but only, say, 10 classes (like MNIST) we can feed the outputs from these forward into a final layer of 10 neurons, and compare those neurons' activation levels.\n",
    "\n",
    "* Essentially we choose the output class whose neuron has the highest activation\n",
    "* To make this mathematically friendly, instead of just using \"argmax\" we calculate the output using something called \"softmax,\" a smoothed/softened version that is normalized to sum to 1:\n",
    "\n",
    "$$\\sigma (\\mathbf {z} )_{j}={\\frac {e^{z_{j}}}{\\sum _{k=1}^{K}e^{z_{k}}}}$$\n",
    "\n",
    "### So our network looks like this:\n",
    "<img src=\"http://i.imgur.com/LqyPRBd.jpg\">\n",
    "\n",
    "* Where we attach our features to the \"input layer\"\n",
    "    * Here imagining 3 features in each input record\n",
    "* Feed those values forward to a sigmoid activation hidden layer\n",
    "* Then feed the activations from the hidden layer to the output layer\n",
    "    * Here imagining 2 possible output classes; technically if there are only 2 output classes, we could get away with one neuron in the output layer, but we normally have one per class\n",
    "* We calculate the softmax vector of the output activations\n",
    "* ...and that's our probability distribution for the \"actual\" predicted output\n",
    "\n",
    "---\n",
    "\n",
    "> __ASIDE: this structure reproduces the same math as multiclass logistic regression__\n",
    "\n",
    "---\n",
    "\n",
    "Ok, before we talk any more theory, let's run it and see if we can do better on our diamonds dataset!\n",
    "\n",
    "Again, we'll hop outside of Jupyter and on the console, using your Python `dl` conda environment and `scripts/` folder, run `python keras-sigmoid.py`\n",
    "\n",
    "While that's running, let's look at the code:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=26, kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What is different here?\n",
    "\n",
    "* First, we've changed the activation in the hidden layer to \"sigmoid\" per our discussion.\n",
    "\n",
    "Note that we didn't have to explicitly write the \"input\" layer, courtesy of the Keras API. We just said `input_dim=26` on the first (and only) hidden layer.\n",
    "\n",
    "* Next, notice that we're running 2000 training epochs!\n",
    "\n",
    "It takes a long time to converge. If you experiment a lot, you'll find that ... it still takes a long time to converge. Around the early part of the most recent deep learning renaissance, researchers started experimenting with other non-linearities.\n",
    "\n",
    "*Output here is still using \"linear\" rather than \"softmax\" because we're performing regression, not classification*\n",
    "\n",
    "In theory, any non-linearity should allow learning, and maybe we can use one that \"works better\"\n",
    "\n",
    "By \"works better\" we mean\n",
    "* Simpler gradient - faster to compute\n",
    "* Less prone to \"saturation\" -- where the neuron ends up way off in the 0 or 1 territory of the sigmoid and can't easily learn anything\n",
    "* Keeps gradients \"big\" -- avoiding the large, flat, near-zero gradient areas of the sigmoid\n",
    "\n",
    "Turns out that the most popular solution is a very simple hack:\n",
    "\n",
    "### Rectified Linear Unit (ReLU)\n",
    "\n",
    "<img src=\"images/activation-functions.svg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Go change your hidden-layer activation from 'sigmoid' to 'relu'\n",
    "\n",
    "Start your script and watch the error for a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Would you look at that?! \n",
    "\n",
    "* We break \\$1000 RMSE around epoch 112\n",
    "* \\$900 around epoch 220\n",
    "* \\$800 around epoch 450\n",
    "* By around epoch 2000, my RMSE is about $620\n",
    "...\n",
    "\n",
    "\n",
    "__Same theory; different activation function. Huge difference__\n",
    "\n",
    "Feel free to experiment with other activation functions. Where would you find the options in Keras? How could you experiment with a custom activation functions?\n",
    "\n",
    "---\n",
    "\n",
    "### Some things to think about...\n",
    "\n",
    "1. Consider the shape of the (sub)spaces that these neurons can \"carve out\"\n",
    "    * What is the shape like compared to the sigmoid version? the decision tree version? think about the edges\n",
    "2. ReLU is a bit like a logic gate -- an if/else conditional\n",
    "3. ReLU supports more sparsity -- inactive neurons are just zero -- a form of feature selection\n",
    "4. Since the pre-activations are linear, and the activations are non-linear, these sorts of models have been compared to a form of GLM (generalized linear model) where the activation takes the place of the \"link function\"\n",
    "5. This is a high capacity model if we add lots of neurons\n",
    "    * But it will have a wide & shallow shape \n",
    "    * ... we will move on soon to motivate deeper networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
