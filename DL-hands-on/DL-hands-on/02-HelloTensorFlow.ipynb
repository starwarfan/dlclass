{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow\n",
    "\n",
    "### ... is a general math framework\n",
    "\n",
    "TensorFlow is designed to accommodate...\n",
    "\n",
    "* Easy operations on tensors (n-dimensional arrays)\n",
    "* Mappings to performant low-level implementations, including native CPU and GPU\n",
    "* Optimization via gradient descent variants\n",
    "    * Including high-performance differentiation\n",
    "    \n",
    "Low-level math primitives called \"Ops\"\n",
    "\n",
    "From these primitives, linear algebra and other higher-level constructs are formed.\n",
    "\n",
    "Going up one more level common neural-net components have been built and included.\n",
    "\n",
    "At an even higher level of abstraction, various libraries have been created that simplify building and wiring common network patterns. Over the last year, we've seen 3-5 such libraries.\n",
    "\n",
    "We will focus later on one, Keras, which has now been adopted as the \"official\" high-level wrapper for TensorFlow.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### We'll get familiar with TensorFlow so that it is not a \"magic black box\"\n",
    "\n",
    "But for most of our work, it will be more productive to work with the higher-level wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(100, name='x')\n",
    "y = tf.Variable(x + 50, name='y')\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### There's a bit of \"ceremony\" there...\n",
    "\n",
    "... and ... where's the actual output?\n",
    "\n",
    "For performance reasons, TensorFlow separates the design of the computation from the actual execution.\n",
    "\n",
    "TensorFlow programs describe a computation graph -- an abstract DAG of data flow -- that can then be analyzed, optimized, and implemented on a variety of hardware, as well as potentially scheduled across a cluster of separate machines.\n",
    "\n",
    "Like many query engines and compute graph engines, evaluation is __lazy__ ... so we don't get \"real numbers\" until we force TensorFlow to run the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorFlow integrates tightly with NumPy\n",
    "\n",
    "and we typically use NumPy to create and manage the tensors (vectors, matrices, etc.) that will \"flow\" through our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.randint(1000, size=100)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = np.random.normal(loc=10.0, scale=2.0, size=[3,3]) # mean 10, std dev 2\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(data, name='x')\n",
    "y = tf.Variable(x * 10, name='y')\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### We will often iterate on a calculation ... \n",
    "\n",
    "Calling `session.run` runs just one step, so we can iterate using Python as a control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    for i in range(3):\n",
    "        session.run(model)\n",
    "        x = x + 1\n",
    "        print(session.run(x))\n",
    "        print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TensorBoard is a helper tool for visualizing compute graphs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(100, name='x')\n",
    "\n",
    "print(x)\n",
    "\n",
    "y = tf.Variable(x + 1, name='y')\n",
    "\n",
    "with tf.Session() as session:\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"data/scratch\", session.graph)\n",
    "    model =  tf.global_variables_initializer()\n",
    "    session.run(model)\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code records a log file... To view it, run TensorBoard separately from the command line:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=data/scratch/\n",
    "\n",
    "Starting TensorBoard 39 on port 6006\n",
    "```\n",
    "\n",
    "And browse to `localhost:6006` (or whichever port is selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3]})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [2, 3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, 2]) #None --> unspecified\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2], [3, 4], [5, 6]]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Let's make a slightly more complex graph:\n",
    "\n",
    "We'll use a slice operator (https://www.tensorflow.org/api_docs/python/tf/slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [2, 3])\n",
    "y = x * 2\n",
    "z = tf.slice(y, [0,1], [2,2]) * 10\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(z, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What does the compute graph for this look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "x = tf.constant(list(range(10)))\n",
    "\n",
    "print(x)\n",
    "print(x.eval())\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Optimizers\n",
    "\n",
    "TF includes a set of built-in algorithm implementations (though you could certainly write them yourself) for performing optimization.\n",
    "\n",
    "These are oriented around a gradient-descent methods, with a set of handy extensions flavors to make things converge faster.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "A family of numeric optimization techniques, where we solve a problem with the following pattern:\n",
    "\n",
    "1. Formulate the goal as a function of many parameters, where we would like to minimize the function's value\n",
    "<br><br>*For example, if we can write the error (e.g., RMSE or cross-entropy) as a function of variables, we would like to find values for those variables that will minimize the error.*<br><br>\n",
    "\n",
    "2. Calculate the target function value\n",
    "\n",
    "3. Compute the gradient, or directional derivative, of the target -- the \"slope toward lower error\"\n",
    "\n",
    "4. Adjust the input variables in the indicated direction\n",
    "\n",
    "5. Repeat\n",
    "\n",
    "<img src=\"http://i.imgur.com/ntIU6Q8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Big Picture Goal: Function Approximation\n",
    "\n",
    "If we can imagine our goal in terms of a function: a value that comes out based on values in, or a probability distribution out from a sampled distribution going in...\n",
    "\n",
    "... and we can formulate our error as something solvable by gradient descent ...\n",
    "\n",
    "... then we can use a numerical solving technique like this to get us plausible values for the parameters of the function.\n",
    "\n",
    "__One more time, because this is important: we're not solving for the output of the function -- in order to \"learn\" a function we need real input and output to begin with. We're solving for the parameters of the the function that that get us close to the output__\n",
    "\n",
    "What about the structure of the function? Where does that come in? That is the going to be our hard-coded hypothesis that we start with. For example, earlier we imagined Logistic Regression as a type of model -- in that case, the logistic regression itself is the structure of the function. We proposed it as a hypothesis, knowing that it would be good at some distributions and bad at others.\n",
    "\n",
    "__As we get further into Deep Learning, we'll see that the neural net topology or type is the structure of the function. As hard as it is to conjecture what might work, it can also get pragmatically hard to solve for all the function parameters because we may have tens or hundreds of thousands of interrelated parameters!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Notes and Thoughts on Gradient Descent\n",
    "\n",
    "We want function approximation in a trainable way. Trainable by gradient descent means the shape of the space we're optimizing should ideally be smooth. It would be great if we knew it was convex or had a unique minimum but those are rarely true, and we can try to get an \"ok\" solution anyway.\n",
    "\n",
    "That's where the research and experimentation comes in.\n",
    "\n",
    "#### Some ideas to help build your intuition\n",
    "\n",
    "* What happens if the variables (imagine just 2, to keep the mental picture simple) are on wildly different scales ... like one ranges from -1 to 1 while another from -1e6 to +1e6?\n",
    "\n",
    "* What if some of the variables are correlated? I.e., a change in one corresponds to, say, a linear change in another?\n",
    "\n",
    "* Other things being equal, an approximate solution with fewer variables is easier to work with than one with more -- how could we get rid of some less valuable parameters? (e.g., L1 penalty)\n",
    "\n",
    "* How do we know how far to \"adjust\" our parameters with each step?\n",
    "\n",
    "<img src=\"http://i.imgur.com/AvM2TN6.png\" width=600>\n",
    "\n",
    "\n",
    "What if we have billions of data points? Does it makes sense to use all of them for each update? Is there a shortcut?\n",
    "\n",
    "Yes: __Stochastic Gradient Descent__\n",
    "\n",
    "### Beyond SGD\n",
    "\n",
    "In the beginning of the big-data machine learning revolution, SGD was the workhorse of optimization. \n",
    "\n",
    "It works, but there are a variety of refinements that have been created so that now, in production, we typically use a slightly more complext variant.\n",
    "\n",
    "#### Momentum\n",
    "\n",
    "* We may want to use some weighted history of our gradient descent path, to smooth out changes in direction and velocity\n",
    "\n",
    "#### Conjugate-Gradient\n",
    "\n",
    "* We can use a change of basis to minimize the amount of time we go back and forth \"undoing\" progress in a a particular dimension ... i.e., we can try to approach the goal in a more additive way.\n",
    "\n",
    "<img src=\"http://i.imgur.com/Jx8YKDc.jpg\" width=\"500\">\n",
    "\n",
    "\n",
    "#### Second-Order Methods\n",
    "\n",
    "* Inspired by Newton's method of approximation\n",
    "* Use 2nd derivatives ... ${n^2}$ second derivatives in the Hessian matrix, so we need some tricks to keep this tractable\n",
    "* L-BFGS, OWL-QN\n",
    "\n",
    "\n",
    "<img src=\"http://i.imgur.com/SZxMEg0.png\" width=200>\n",
    "\n",
    "---\n",
    "\n",
    "We'll try and talk about some of the more \"advanced\" tricks later, but just so the terminology isn't mysterious, these are things like *Adam*, *Adagrad*, *Adadelta*, *RMSProp* etc.\n",
    "\n",
    "You definitely don't need to be able to code all of those by hand, but it will be useful having a bit of intuition about what the idea is, so you can choose -- or understand why another scientist chose -- a particular optimizer and parameters. In reality, a lot of time is based on empirical experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Back to TensorFlow: Using TF optimizer to solve problems\n",
    "\n",
    "As we've said, TF is a toolkit for math, and it has higher-level implementations (like gradient descent optimizers) that are available to us.\n",
    "\n",
    "We can use them to solve anything (not just neural networks) so let's start with a simple equation.\n",
    "\n",
    "We supply a bunch of data points, that represent inputs. We will generate them based on a known, simple equation (y will always be 2\\*x + 6) but we won't tell TF that. Instead, we will give TF a function structure ... linear with 2 parameters, and let TF try to figure out the parameters by minimizing an error function.\n",
    "\n",
    "What is the error function? \n",
    "\n",
    "The \"real\" error is the absolute value of the difference between TF's current approximation and our ground-truth y value.\n",
    "\n",
    "But absolute value is not a friendly function to work with there, so instead we'll square it. That gets us a nice, smooth function that TF can work with, and it's just as good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\")\n",
    "y = tf.placeholder(\"float\")\n",
    "\n",
    "m = tf.Variable([1.0], name=\"m-slope-coefficient\") # initial values ... for now they don't matter much\n",
    "b = tf.Variable([1.0], name=\"b-intercept\")\n",
    "\n",
    "y_model = tf.multiply(x, m) + b\n",
    "\n",
    "error = tf.square(y - y_model)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(error)\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    for i in range(10):\n",
    "        x_value = np.random.rand()\n",
    "        y_value = x_value * 2 + 6\n",
    "        session.run(train_op, feed_dict={x: x_value, y: y_value})\n",
    "\n",
    "    out = session.run([m, b])\n",
    "    print(out)\n",
    "    print(\"Model: {r:.3f}x + {s:.3f}\".format(r=out[0][0], s=out[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### That's pretty terrible :)\n",
    "\n",
    "Try two experiments. Change the number of iterations the optimizer runs, and -- independently -- try changing the learning rate (that's the number we passed to `GradientDescentOptimizer`)\n",
    "\n",
    "See what happens with different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### We can also look at the errors and plot those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\")\n",
    "y = tf.placeholder(\"float\")\n",
    "\n",
    "m = tf.Variable([1.0], name=\"m-slope-coefficient\") # initial values ... for now the don't matter much\n",
    "b = tf.Variable([1.0], name=\"b-intercept\")\n",
    "\n",
    "y_model = tf.multiply(x, m) + b\n",
    "\n",
    "error = tf.square(y - y_model)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(error)\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "errors = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    for i in range(100):\n",
    "        x_value = np.random.rand()\n",
    "        y_value = x_value * 2 + 6\n",
    "        _, error_val = session.run([train_op, error], feed_dict={x: x_value, y: y_value})\n",
    "        errors.append(error_val)\n",
    "\n",
    "    out = session.run([m, b])\n",
    "    print(out)\n",
    "    print(\"Model: {r:.3f}x + {s:.3f}\".format(r=out[0][0], s=out[1][0]))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### That is the essence of TensorFlow!\n",
    "\n",
    "There are three principal directions to explore further:\n",
    "\n",
    "* Working with tensors instead of scalars: this is not intellectually difficult, but takes some practice to wrangle the shaping and re-shaping of tensors. If you get the shape of a tensor wrong, your script will blow up. Just takes practice.\n",
    "\n",
    "* Building more complex models. You can write these yourself using lower level \"Ops\" -- like matrix multiply -- or using higher level classes like `tf.layers.dense` *Use the source, Luke!*\n",
    "\n",
    "* Operations and integration ecosystem: as TensorFlow has matured, it is easier to integrate additional tools and solve the peripheral problems:\n",
    "    * TensorBoard for visualizing training\n",
    "    * tfdbg command-line debugger\n",
    "    * Distributed TensorFlow for clustered training\n",
    "    * GPU integration\n",
    "    * Feeding large datasets from external files\n",
    "    * Tensorflow Serving for serving models (i.e., using an existing model to predict on new incoming data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
