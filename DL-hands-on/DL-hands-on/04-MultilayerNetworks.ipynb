{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multilayer Networks\n",
    "\n",
    "If a single-layer perceptron network learns the importance of different combinations of features in the data...\n",
    "\n",
    "What would another network learn if it had a second (hidden) layer of neurons?\n",
    "\n",
    "It depends on how we train the network. We'll talk in the next section about how this training works, but the general idea is that we still work backward from the error gradient. \n",
    "\n",
    "That is, the last layer learns from error in the output; the second-to-last layer learns from error transmitted through that last layer, etc. It's a touch hand-wavy for now, but we'll make it more concrete later.\n",
    "\n",
    "Given this approach, we can say that:\n",
    "\n",
    "1. The second (hidden) layer is learning features composed of activations in the first (hidden) layer\n",
    "2. The first layer is (ideally) learning feature weights that enable the second layer to perform best \n",
    "    * Why? Earlier, the first hidden layer just learned feature weights because that's how it was judged\n",
    "    * Now, the first hidden layer is judged on the error in the second layer, so it learns to contribute to that second layer\n",
    "3. The second layer is learning new features that aren't explicit in the data, and is teaching the first layer to supply it with the necessary information to compose these new features\n",
    "\n",
    "### So instead of just feature weighting and combining, we have new feature learning!\n",
    "\n",
    "This concept is the foundation of the \"Deep Feed-Forward Network\"\n",
    "\n",
    "<img src=\"http://i.imgur.com/fHGrs4X.png\">\n",
    "\n",
    "---\n",
    "\n",
    "### Let's try it!\n",
    "\n",
    "__Add a layer to your Keras network, perhaps another 20 neurons, and see how the training goes.__\n",
    "\n",
    "if you get stuck, there is a solution in `keras-dffn.py`\n",
    "\n",
    "---\n",
    "\n",
    "I'm getting RMSE < \\$1000 by epoch 28\n",
    "\n",
    "< \\$800 by epoch 100\n",
    "\n",
    "In this configuration, mine makes progress to around 600 epochs or so and then stalls with RMSE around $560\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Our network has \"gone meta\"\n",
    "\n",
    "It's now able to exceed where a simple decision tree can go, because it can create new features and then split on those\n",
    "\n",
    "## Congrats! You have built your first deep-learning model!\n",
    "\n",
    "So does that mean we can just keep adding more layers and solve anything?\n",
    "\n",
    "Well, theoretically maybe ... try reconfiguring your network, watch the training, and see what happens.\n",
    "\n",
    "<img src=\"http://i.imgur.com/BumsXgL.jpg\" width=500>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
