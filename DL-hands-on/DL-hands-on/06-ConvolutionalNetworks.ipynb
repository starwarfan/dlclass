{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "## aka CNN, ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As a baseline, let's start a lab running with what we already know.\n",
    "\n",
    "We'll take our deep feed-forward multilayer perceptron network, with ReLU activations and reasonable initializations, and apply it to learning the MNIST digits.\n",
    "\n",
    "The main part of the code looks like this, we won't run it in the notebook -- it will take too long. Instead, run it in the terminal, where the script is called `mnist-mlp.py`\n",
    "\n",
    "Note the changes, which are largely about building a classifier instead of a regression model:\n",
    "* Output layer has one neuron per category, with softmax activation\n",
    "* Loss function is cross-entropy loss\n",
    "* Accuracy metric is categorical accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports, setup, load data sets\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=784, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(15, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "categorical_labels = to_categorical(y_train, num_classes=10)\n",
    "start = datetime.datetime.today()\n",
    "\n",
    "history = model.fit(X_train, categorical_labels, epochs=100, batch_size=100)\n",
    "\n",
    "# print metrics, plot errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What are the big takeaways from this experiment?\n",
    "\n",
    "1. We get pretty impressive \"apparent error\" accuracy right from the start! A small network gets us to training error 97% by epoch 15 and 98% around epoch 40.\n",
    "2. The model *appears* to continue to learn, although it does slow down and oscillate a bit.\n",
    "3. Our test accuracy is about 95% after 100 iterations and 2.5 minutes or so.\n",
    "4. Therefore, we are overfitting very quickly... most of the \"training\" turns out to be a waste.\n",
    "5. For what it's worth, we get 95% accuracy without much work.\n",
    "\n",
    "This is not terrible compared to other, non-neural-network approaches to the problem. After all, we could probably tweak this a bit and do even better.\n",
    "\n",
    "But we talked about using deep learning to solve \"95%\" problems or \"98%\" problems ... where one error in 20, or 50 simply won't work. If we can get to \"multiple nines\" of accuracy, then we can do things like automate mail sorting and translation, create cars that react properly (all the time) to street signs, and control systems for robots or drones that function autonomously.\n",
    "\n",
    "Try two more experiments (try them separately):\n",
    "1. Add a third, hidden layer.\n",
    "2. Increase the size of the hidden layers.\n",
    "\n",
    "Adding another layer slows things down a little (why?) but doesn't seem to make a difference in accuracy.\n",
    "\n",
    "Adding a lot more neurons into the first topology slows things down significantly -- 10x as many neurons, and only a marginal increase in accuracy. Notice also (in the plot) that the learning clearly degrades after epoch 50 or so.\n",
    "\n",
    "... We need a new approach!\n",
    "\n",
    "---\n",
    "\n",
    "... let's think about this:\n",
    "\n",
    "### What is layer 2 learning from layer 1? Combinations of pixels\n",
    "\n",
    "#### Combinations of pixels contain information but...\n",
    "\n",
    "There are a lot of them (combinations) and they are \"fragile\" \n",
    "\n",
    "In fact, in our last experiment, we basically built a model that memorizes a bunch of \"magic\" pixel combinations.\n",
    "\n",
    "What might be a better way to build features?\n",
    "\n",
    "* When humans perform this task, we look not at arbitrary pixel combinations, but certain geometric patterns -- lines, curves, loops.\n",
    "* These features are made up of combinations of pixels, but they are far from arbitrary\n",
    "* We identify these features regardless of translation, rotation, etc.\n",
    "\n",
    "Is there a way to get the network to do the same thing?\n",
    "\n",
    "I.e., in layer one, identify pixels. Then in layer 2+, identify abstractions over pixels that are translation-invariant shapes?\n",
    "\n",
    "We could look at where a \"filter\" that represents one of these features (e.g., and edge) matches the image.\n",
    "\n",
    "How would this work?\n",
    "\n",
    "### Convolution\n",
    "\n",
    "Convolution in the general mathematical sense is define as follows:\n",
    "\n",
    "${\\begin{aligned}(f*g)(t)&\\,{\\stackrel {\\mathrm {def} }{=}}\\ \\int _{-\\infty }^{\\infty }f(\\tau )\\,g(t-\\tau )\\,d\\tau \\\\&=\\int _{-\\infty }^{\\infty }f(t-\\tau )\\,g(\\tau )\\,d\\tau .\\end{aligned}}$\n",
    "\n",
    "The convolution we deal with in deep learning is a simplified case. We want to compare two signals. Here are two visualizations, courtesy of Wikipedia, that help communicate how convolution emphasizes features:\n",
    "\n",
    "<img src=\"http://i.imgur.com/EDCaMl2.png\" width=500>\n",
    "\n",
    "---\n",
    "\n",
    "#### Here's an animation (where we change ${\\tau}$) \n",
    "<img src=\"http://i.imgur.com/0BFcnaw.gif\">\n",
    "\n",
    "__In one sense, the convolution captures and quantifies the pattern matching over space__\n",
    "\n",
    "If we perform this in two dimensions, we can achieve effects like highlighting edges:\n",
    "\n",
    "<img src=\"http://i.imgur.com/DKEXIII.png\">\n",
    "\n",
    "The matrix here, also called a convolution kernel, is one of the functions we are convolving. Other convolution kernels can blur, \"sharpen,\" etc. There's a great PDF from Cornell University <a href=\"bonus/imaging_and_convolution.pdf\">in the bonus folder</a>.\n",
    "\n",
    "### So we'll drop in a number of convolution kernels, and the network will learn where to use them? Nope. Better than that.\n",
    "\n",
    "## We'll program in the *idea* of discrete convolution, and the network will learn what kernels extract meaningful features!\n",
    "\n",
    "The values in a (fixed-size) convolution kernel matrix will be variables in our deep learning model. Although inuitively it seems like it would be hard to learn useful params, in fact, since those variables are used repeatedly across the image data, it \"focuses\" the error on a smallish number of parameters with a lot of influence -- so it should be vastly *less* expensive to train than just a huge fully connected layer like we discussed above.\n",
    "\n",
    "This idea was developed in the late 1980s, and by 1989, Yann LeCun (at AT&T/Bell Labs) had built a practical high-accuracy system (used in the 1990s for processing handwritten checks and mail).\n",
    "\n",
    "__How do we hook this into our neural networks?__\n",
    "\n",
    "* First, we can preserve the geometric properties of our data by \"shaping\" the vectors as 2D instead of 1D.\n",
    "\n",
    "* Then we'll create a layer whose value is not just activation applied to weighted sum of inputs, but instead it's the result of a dot-product (element-wise multiply and sum) between the kernel and a patch of the input vector (image).\n",
    "    * This value will be our \"pre-activation\" or feed into an activation function\n",
    "\n",
    "<img src=\"http://i.imgur.com/ECyi9lL.png\">\n",
    "\n",
    "* If we perform this operation at lots of positions over the image, we'll get lots of outputs, as many as one for every input pixel. \n",
    "\n",
    "\n",
    "<img src=\"http://i.imgur.com/WhOrJ0Y.jpg\">\n",
    "\n",
    "* So we'll add another layer that \"picks\" the highest convolution pattern match from nearby pixels, which\n",
    "    * makes our pattern match a little bit translation invariant (a fuzzy location match)\n",
    "    * reduces the number of outputs significantly\n",
    "* This layer is commonly called a pooling layer, and if we pick the \"maximum match\" then it's a \"max pooling\" layer.\n",
    "\n",
    "<img src=\"http://i.imgur.com/9iPpfpb.png\">\n",
    "\n",
    "__The end result is that the kernel or filter together with max pooling creates a value in a subsequent layer which represents the appearance of a pattern in a local area in a prior layer.__\n",
    "\n",
    "__Again, the network will be given a number of \"slots\" for these filters and will learn (by minimizing error) what filter values produce meaningful features. This is the key insight into how modern image-recognition networks are able to generalize -- i.e., learn to tell 6s from 7s or cats from dogs.__\n",
    "\n",
    "<img src=\"http://i.imgur.com/F8eH3vj.png\">\n",
    "\n",
    "## Ok, let's build our first ConvNet:\n",
    "\n",
    "(run the script from the console, `mnist-cnn1.py`)\n",
    "\n",
    "First, we want to explicity shape our data into a 2-D configuration. We'll end up with a 4-D tensor where the first dimension is the training examples, then each example is 28x28 pixels, and we'll explicitly say it's 1-layer deep. (Why? with color images, we typically process over 3 or 4 channels in this last dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_libsvm = \"../data/mnist\"\n",
    "\n",
    "X_train, y_train = sklearn.datasets.load_svmlight_file(train_libsvm, n_features=784)\n",
    "X_train = X_train.toarray()\n",
    "X_train = X_train.reshape( (X_train.shape[0], 28, 28, 1) )\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255\n",
    "y_train = to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# setup, imports, load and shape\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(8, # number of kernels \n",
    "                        (4, 4), # kernel size\n",
    "                        padding='valid',\n",
    "                        input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While that's running, let's look at a number of \"famous\" convolutional networks!\n",
    "\n",
    "### LeNet (Yann LeCun, 1998)\n",
    "\n",
    "<img src=\"http://i.imgur.com/k5hMtMK.png\">\n",
    "\n",
    "<img src=\"http://i.imgur.com/ERV9pHW.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### AlexNet (2012)\n",
    "\n",
    "<img src=\"http://i.imgur.com/CpokDKV.jpg\">\n",
    "\n",
    "<img src=\"http://i.imgur.com/Ld2QhXr.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Our MNIST ConvNet\n",
    "\n",
    "In our first convolutional MNIST experiment, we get to 98.75% in 20 epochs (a couple of minutes on CPU)!\n",
    "\n",
    "The training error is 99.89% so we've almost completely overfit by this point and need to do a little work if we want to keep learning.\n",
    "\n",
    "Let's add another convolutional layer (script is in `mnist-cnn2.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(8, # number of kernels \n",
    "\t\t\t\t\t\t(4, 4), # kernel size\n",
    "                        padding='valid',\n",
    "                        input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(8, (4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While that's running, let's look at some more recent ConvNet architectures:\n",
    "\n",
    "### VGG16 (2014)\n",
    "\n",
    "<img src=\"http://i.imgur.com/gl4kZDf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### GoogLeNet (2014)\n",
    "\n",
    "<img src=\"http://i.imgur.com/hvmtDqN.png\">\n",
    "\n",
    "*\"Inception\" layer: parallel convolutions at different resolutions*\n",
    "\n",
    "<img src=\"http://i.imgur.com/TCN9C4P.png\">\n",
    "\n",
    "### Residual Networks (2015-)\n",
    "\n",
    "Skip layers to improve training (error propagation). Residual layers learn from details at multiple previous layers.\n",
    "\n",
    "<img src=\"http://i.imgur.com/32g8Ykl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Back to our labs: Seriously Overfitting\n",
    "\n",
    "We're making progress on our test error, but just a bit compared to the time, due to the network overfitting the data.\n",
    "\n",
    "There are a variety of techniques we can take to counter this, discussed earlier. Let's try a relatively simple solution - add a Dropout filter (`mnist-cnn3.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(8, # number of kernels \n",
    "\t\t\t\t\t\t(4, 4), # kernel size\n",
    "                        padding='valid',\n",
    "                        input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(8, (4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Practicalities Part II\n",
    "## Summary of Optimizer Variants\n",
    "\n",
    "(Although these techniques are all open and discussed in various research papers, for this section, I credit a great concise blog post summarizing the techniques, by Sebastian Ruder, from which I've excerpted some graphics and content: http://sebastianruder.com/optimizing-gradient-descent/index.html -- great researcher and contributer to the community! Many thanks!)\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Keep some (decaying) amount of prior update direction, and mix with current update step, to damp oscillations:\n",
    "\n",
    "<img src=\"http://i.imgur.com/XSUHIfm.png\" width=600>\n",
    "\n",
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "Use our momentum term to \"look ahead\" at future parameter values and approximate future gradient.\n",
    "\"NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness...\"\n",
    "\n",
    "<img src=\"http://i.imgur.com/l7Efbf8.png\" width=600>\n",
    "\n",
    "### Adagrad (Adaptive Gradient)\n",
    "\n",
    "Tune the learning rate automatically based on the gradient. The learning rate is adapted component-wise, and is given by the square root of sum of squares of the historical, component-wise gradient.\n",
    "\n",
    "\"Adagrad's main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.\"\n",
    "\n",
    "<img src=\"http://i.imgur.com/B7eAmPl.gif\">\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size ${w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### *Lab Results -- Looking Good*\n",
    "\n",
    "The last lab should get you to 99% test error after 15 epochs!\n",
    "\n",
    "Try one more experiment, with the following changes:\n",
    "\n",
    "* Make your convolution filters 3x3\n",
    "* Add a 25% dropout before the Flatten'ed Dense layer\n",
    "* Use 32 convolution filters in each convolution layer\n",
    "* Run for 12 epochs\n",
    "\n",
    "(solution is in `mnist-cnn4.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RMS Prop\n",
    "\n",
    "RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class (http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).\n",
    "\n",
    "\"RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta ... RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests ${\\gamma}$ to be set to 0.9, while a good default value for the learning rate ${\\eta}$ is 0.001.\"\n",
    "\n",
    "### Adam\n",
    "\n",
    "\"Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients similar to momentum.\"\n",
    "\n",
    "Retained estimates are the first moment (the mean) and the second moment (the uncentered variance) of the gradients.\n",
    "Bias-corrected first and second moment estimates are used to update the parameters as in Adadelta and RMSprop.\n",
    "\n",
    "<img src=\"http://i.imgur.com/EGZ43p6.gif\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## *Lab Wrapup*\n",
    "\n",
    "From the last lab, you should have a test error after 12 epochs of about 0.86%\n",
    "\n",
    "For one more activity, try changing the optimizer to old-school \"sgd\" -- just to see how far we've come with these modern gradient descent techniques in the last few years.\n",
    "\n",
    "About 96.3% test accuracy after 12 epochs. Two key takeaways:\n",
    "\n",
    "* Without a good optimizer, even a very powerful network design may not achieve results\n",
    "* In fact, we could replace the word \"optimizer\" there with\n",
    "    * initialization\n",
    "    * activation\n",
    "    * regularization\n",
    "    * (etc.)\n",
    "* All of these elements we've been working with operate together in a complex way to determine final performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
